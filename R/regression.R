#' Summarize the results from a regression analysis
#'
#' Alternative to `summary.lm` to summarize a regression from `lm`.
#' Prints a table similar to the one generated by SAS and Minitab.
#' @param lmobject a fitted regression model from `lm`.
#' @param anova `TRUE` if an ANOVA table is computed.
#' @param fit_measures `TRUE` if measures of fit (RÂ² etc) is computed.
#' @param param `TRUE` if parameter estimates, standard errors etc is computed.
#' @param conf_intervals `TRUE` if confidence intervals for parameters.
#' @param vif_factors `TRUE` if variance inflation factors are to be printed.
#' @return list with three tables: param, anova and fit_measures
#' @export
#' @examples
#' library(sda1)
#' lmfit = lm(nRides ~ temp + hum + windspeed, data = bike)
#' regsumm = reg_summary(lmfit, anova = T, conf_intervals = T, vif_factors = T)
#' regsumm$param
#' regsumm$anova
#' regsumm$fit_measures
reg_summary <- function(lmobject, anova = T,  fit_measures = T, param = T,
                       conf_intervals = F, vif_factors = F){

  if ("(Intercept)" %in% names(lmobject$coefficients)) intercept = 1 else intercept = 0

  lmsummary = summary(lmobject)
  df_regr = lmsummary$df[1] - intercept
  df_error = lmsummary$df[2]
  df_total = df_error + df_regr
  sse = (lmsummary$sigma^2)*df_error
  sst = var(lmobject$model[,1])*df_total
  ssr = sst - sse

  # Anova table
  anova_table = NA
  if (anova){
    anova_table = matrix(rep(NA,5*3),3,5)
    anova_table[,1] = c(df_regr,df_error,df_total)
    anova_table[,2] = c(ssr,sse,sst)
    anova_table[1:2,3] = c(ssr/df_regr, sse/df_error)
    anova_table[1,4] = lmsummary$fstatistic[1]
    anova_table[1,5] = pf(lmsummary$fstatistic[1], lmsummary$fstatistic[2], lmsummary$fstatistic[3], lower.tail = F)
    rownames(anova_table) <- c("Regr","Error","Total")
    colnames(anova_table) <- c("df","SS","MS","F","Pr(>F)")
    {cat("\nAnalysis of variance - ANOVA\n------------------------------------------------\n");
    print(anova_table, digits = 5, na.print = "")}
  }

  fit_table = NA
  if (fit_measures){
    fit_table = c(sqrt(sse/df_error), lmsummary$r.squared, lmsummary$adj.r.squared)
    names(fit_table) <- c("Root MSE","R2","R2-adj")
    {cat("\nMeasures of model fit\n------------------------------------------------\n");
    print(fit_table, digits = 5, na.print = "")}
  }

  # Table with estimated coefficients etc
  if (param){
    # Confidence intervals on parameters
    if (conf_intervals){
      param_table = cbind(lmsummary$coefficients, confint(lmobject))
    }else
    {
      param_table = lmsummary$coefficients
    }

    # Variance inflation factors
    if (vif_factors & df_regr>1){
      data = model.matrix(lmobject)
      X = data[,-1]
      vif = rep(NA,df_regr)
      for (j in 1:df_regr){
        vif[j] = 1/(1-summary(lm(X[,j] ~ data.matrix(X[,-j])))$r.squared)
      }
      if (intercept) vif = c(NA,vif)
      param_table = cbind(param_table,vif)
      colnames(param_table)[ncol(param_table)] = "VIF"
    }

    {cat("\nParameter estimates\n------------------------------------------------\n");
    print(param_table, digits = 5, na.print = "")}

  }else{param_table = NA}

  invisible(list(param = param_table, anova = anova_table, fit_measures = fit_table))
}

#' Plot confidence and prediction intervals for simple linear regression \cr
#'
#' @param formula an object of class "formula": a symbolic description of the model to be fitted.
#' @param data a data frame with the data.
#' @param level confidence level, default is level = 0.95
#' @param conf_int_line if TRUE, then conf intervals for regression line are plotted.
#' @param pred_interval if TRUE, then prediction intervals are plotted.
#' @return plot of data with overlayed intervals
#' @export
#' @examples
#' library(sda1)
#' reg_predict(mpg ~ hp, data = mtcars)
reg_predict <- function(formula, data, level = 0.95,
                             conf_int_line = T, pred_interval = T){

  fit <- lm(formula, data = data)
  data <- cbind(data, suppressWarnings(
                  predict(fit, data = data, interval = "prediction",
                  level = level)))

  p <- ggplot(data, aes(x = .data[[names(fit$model)[2]]],
                        y = .data[[names(fit$model)[1]]])) +
    {if (pred_interval) geom_ribbon(aes(ymin = lwr, ymax = upr),
                          fill = prettycol[1], alpha = 0.3)
    } +
    {if (conf_int_line) suppressMessages(stat_smooth(method = "lm",
                          se = T, level = level,
                          color = prettycol[4], fill = prettycol[2]))
    else{geom_abline(aes(intercept = fit$coefficients[1],
                           slope = fit$coefficients[2]),
                           color = prettycol[2])}
    } +
    geom_point(color = prettycol[2], size = 1) +
    ggtitle("Konfidens- och prediktionsintervall") +
    theme_light() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
  return(p)
}

#' Simulate from a linear regression model
#'
#' Simulates a dataset with `n` observation from the linear regression model
#' \deqn{y = \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k + \epsilon, \epsilon \sim N(0, \sigma_\epsilon^2)}{y = \beta_0 + \beta_1 * x_1 + ... + \beta_k * x_k + \epsilon, \epsilon ~ N(0, \sigma_ \epsilon^2)}
#' with covariates (x) simulated from a normal distribution with the same correlation `rho_x` \cr
#' between all pairs of covariates. Covariate \eqn{x_j}{x_j} has standard deviation `sigma_x[j]`. \cr
#' Alternatively the covariate can follow a uniform distribution.
#' @param n the number of observations in the simulated dataset.
#' @param betavect a vector with regression coefficients
#' c(beta_0,beta_1,...beta_k). First element is intercept if `intercept = TRUE`
#' @param sigma_eps standard deviation of the error terms, epsilon.
#' @param intercept if `TRUE` an intercept is added to the model.
#' @param covdist distribution of the covariates. Options: `'normal'` or `'uniform'`.
#' @param rho_x correlation among the covariates. Same for all covariate pairs.
#' @param sigma_x vector with standard deviation of the covariates.
#' @return dataframe with simulated data (y, X1, X2, ..., XK) (no intercept included).
#' @export
#' @examples
#' library(sda1)
#' simdata <- reg_simulate(n = 500, betavect = c(1, -2, 1, 0), sigma_eps = 2)
#' lmfit <- lm(y ~ X1 + X2 + X3, data = simdata)
#' reg_summary(lmfit, anova = F)
reg_simulate <- function(n, betavect, sigma_eps, intercept = TRUE, covdist = 'normal',
                        rho_x = 0, sigma_x = rep(1,length(betavect)-intercept)){

  k = length(betavect) - intercept

  # Generate covariates
  if (covdist == 'normal'){
    rho = matrix(rho_x, k, k)
    diag(rho) <- 1
    sigma = diag(sigma_x)%*%rho%*%diag(sigma_x)
    X = mvtnorm::rmvnorm(n, sigma = sigma)
  }else{
    X = matrix(runif(n*k), n, k)
    if (rho_x != 0) warning("uniformly distributed covariates are always uncorrelated")
  }
  if (intercept) X = cbind(1,X)

  # Simulate responses
  y = X%*%betavect + rnorm(n, sd = sigma_eps)

  if (intercept) X = X[,-1] # remove intercept in the returned dataset
  data = data.frame(cbind(y,X))
  xnames = rep(NA,k)
  for (j in 1:k) xnames[j] = paste("X",j, sep = "")
  names(data) <- c("y", xnames)
  return(data)
}

#' K-fold cross-validation of regression models estimated with lm()
#'
#' @param formula an object of class "formula": a symbolic description of the model to be fitted.
#' @param data a data frame with the data used for fitting the models.
#' @param nfolds the number of folds in the cross-validation.
#' @param obs_order order of the observations when splitting the data. obs_order = "random" gives a random order.
#' @return RMSE Root mean squared prediction error on test data
#' @export
#' @examples
#' library(sda1)
#' RMSE_CV = reg_crossval(mpg ~ hp, data = mtcars, nfolds = 4, obs_order = 1:32)
#' print(RMSE_CV)
reg_crossval <- function(formula, data, nfolds, obs_order = "random"){

  n = dim(data)[1]
  if (is.character(obs_order)) obs_order = sample(1:n)

  obs_per_fold = ceiling(n/nfolds)
  yhat = matrix(NA, obs_per_fold, nfolds)
  if (n %% nfolds == 0){
    test_obs_matrix = matrix(obs_order, obs_per_fold) # k:th column contains test for fold k
  }else{
    nobs_last_fold = n-obs_per_fold*(nfolds-1)
    test_obs_matrix = matrix(NA, obs_per_fold, nfolds)
    test_obs_matrix[,1:(nfolds-1)] = matrix(obs_order[obs_per_fold*(nfolds-1)], obs_per_fold)
    test_obs_matrix[1:nobs_last_fold, nfolds] = obs_order[(obs_per_fold*(nfolds-1)+1):n]
  }
  for (k in 1:nfolds){
    testfold = test_obs_matrix[,k][!is.na(test_obs_matrix[,k])]
    trainingfold = setdiff(obs_order,testfold)
    fit = lm(formula, data = data[trainingfold,])
    yhat[1:length(testfold),k] = predict(fit, newdata = data[testfold,])
  }
  yhat = c(yhat)[!is.na(c(yhat))]
  yordered = data[all.names(formula)[2]][obs_order,]
  RMSE = sqrt(sum((yordered - yhat)^2)/n)
}

#' Residual analysis mimicing the 4-in-1 plots from Minitab
#'
#' Plots:
#' 1) Normal QQ-plot
#' 2) Residuals vs fitted values
#' 3) Histogram and normal density fit
#' 4) Residuals vs order.
#' @param lm_object a fitted regression model from `lm`.
#' @export
#' @examples
#' library(sda1)
#' fit = lm(mpg ~ hp, data = mtcars)
#' residuals4in1(fit)

residuals4in1 <- function(lm_object){

  n = dim(lm_object$model)[1]

  # adding residuals and fit to the data frame
  lm_object$model$residuals <- resid(lm_object)
  lm_object$model$fitted <- fitted(lm_object)
  lm_object$model$obsnumber <- 1:n

  p1 = lm_object$model %>%
    ggplot(aes(sample = residuals)) +
    stat_qq_line(color = prettycol[4]) +
    stat_qq(color = prettycol[2], size = 1) +
    ylab("residuals") +
    theme_light()

  p2 = lm_object$model %>%
    ggplot(aes(x = fitted, y = residuals)) +
    geom_hline(aes(yintercept = 0), color = prettycol[4]) +
    geom_point(color = prettycol[2], size = 1) +
    xlab("fitted value") +
    ylab("residual") +
    theme_light() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())


  p3 = lm_object$model %>%
    ggplot(aes(x = residuals)) +
    geom_histogram(aes(y = after_stat(density)),
                   bins = 1 + log(n,2), fill = prettycol[1]) +
    geom_density(kernel = "gaussian", color = prettycol[4]) +
    theme_light() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

  p4 = lm_object$model %>%
    ggplot(aes(x = obsnumber, y = residuals)) +
    geom_hline(aes(yintercept = 0), color = prettycol[4]) +
    geom_path(color = prettycol[1]) +
    geom_point(color = prettycol[2], size = 1)  +
    xlab("observation number") +
    theme_light() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

  plot_grid(p1, p2, p3, p4)

  #plt <- list(p1,p2,p3,p4)
}



